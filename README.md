# Transformer from Scratch (Function-Based, NumPy + Math)

This repository contains a **function-based Transformer model implemented from scratch** using only **NumPy and Python's math library**.  
It avoids relaying too much on deep learning frameworks like PyTorch/TensorFlow to give a **deeper understanding of how Transformers work** at the lowest level.

## Features
- **Function-Based Implementation**: Modular and easy to understand  
- **Tokenization**: Uses MBart50TokenizerFast for sub-word tokenization  
- **Embeddings**: Implements custom embedding layers  
- **Attention Mechanism**: Self-attention and Cross-attention manually implemented  
- **Transformer Architecture**: Function-based encoder-decoder manually implemented
- **Helper Functions**: like Positional Encoding,Softmax,Masking are also manually implemented   

## Installation
Clone the repository:
```bash
git clone https://github.com/yourusername/transformer-from-scratch.git
cd transformer-from-scratch
